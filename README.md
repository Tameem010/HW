# HW
The "Attention is All You Need" paper, from 2017, introduced the Transformer model,  in neural networks. It uses attention mechanisms for processing input sequences . 
1- The function of attention can be described as focusing on the weights of words in sentence contexts to obtain correct results
2-in Embedding convert token to vectors for easy handling 
3- Positional Encoding : Adding or modifying some symbols to the inputs so that the model can understand them more clearly 
there use the (cos for even)(sin for odd)
this what i hade understand from peper 
